{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1819e1b6",
   "metadata": {},
   "source": [
    "# Evaluating LLM performance in the presence of missing outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc01432",
   "metadata": {},
   "source": [
    "### *Research goal: use agentic LLMs to calculate RAI frailty scores with only clinical notes extracted from electronic health records*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf58fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "import statsmodels.api as sm\n",
    "\n",
    "all_df = pd.read_csv(r'C:\\Users\\yens01\\Projects\\Frailty\\frailty_allraiscores.csv', index_col=0)\n",
    "if 'age' not in all_df.columns:\n",
    "    print(\"Age column not found, generating random ages for demonstration.\")\n",
    "    all_df['age'] = np.random.randint(18, 100, size = len(all_df))\n",
    "all_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8c339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_model_df(df, model_name):\n",
    "    subset_df = df[['age', 'token_count', 'Validated_RAI_Score', model_name]].copy()\n",
    "    subset_df.rename(columns={model_name: 'llm_RAI_score'}, inplace=True)\n",
    "    return subset_df\n",
    "\n",
    "# extract only columns relevant to llama3.1 single for this demonstration\n",
    "scores_df = get_specific_model_df(all_df, 'llama3_1_single_RAI')\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n          = len(scores_df)\n",
    "n_missing  = sum(scores_df['llm_RAI_score'].isna())\n",
    "n_complete = n - n_missing\n",
    "\n",
    "print(f\"Number of complete cases: {n_complete}, Number of missing cases: {n_missing}, Total: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_missing  = scores_df[\"llm_RAI_score\"].isna()\n",
    "mask_complete = scores_df[\"llm_RAI_score\"].notna()\n",
    "\n",
    "validated_missing  = scores_df.loc[mask_missing,  \"Validated_RAI_Score\"]\n",
    "validated_complete = scores_df.loc[mask_complete, \"Validated_RAI_Score\"]\n",
    "llm_missing        = scores_df.loc[mask_missing,  \"llm_RAI_score\"] # just NaNs lol\n",
    "llm_complete       = scores_df.loc[mask_complete, \"llm_RAI_score\"]\n",
    "\n",
    "scores_df['validated_frailty'] = (scores_df['Validated_RAI_Score'] >= 21)\n",
    "scores_df['llm_frailty']       = (scores_df['llm_RAI_score'] >= 21)\n",
    "validated_complete_frailty     = scores_df.loc[mask_complete, 'validated_frailty']\n",
    "llm_complete_frailty           = scores_df.loc[mask_complete, 'llm_frailty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ad6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_binary_metrics(y_true, y_pred):\n",
    "    accuracy  = accuracy_score(y_true = y_true, y_pred = y_pred)\n",
    "    precision = precision_score(y_true = y_true, y_pred = y_pred)\n",
    "    recall    = recall_score(y_true = y_true, y_pred = y_pred)\n",
    "    fone      = f1_score(y_true = y_true, y_pred = y_pred)\n",
    "    prcurve  = precision_recall_curve(y_true = y_true, probas_pred = y_pred,\n",
    "                                      pos_label = True)\n",
    "    precisioncurve, recallcurve, _ = prcurve\n",
    "    prauc = auc(x = recallcurve, y = precisioncurve)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': fone,\n",
    "        'pr_auc': prauc\n",
    "    }\n",
    "\n",
    "binary_metrics_complete = calc_binary_metrics(\n",
    "    y_true = validated_complete_frailty,\n",
    "    y_pred = llm_complete_frailty\n",
    ")\n",
    "binary_metrics_complete = {k: round(v, 4) for k, v in binary_metrics_complete.items()}\n",
    "print(\"Binary metrics for complete cases:\")\n",
    "print(binary_metrics_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb147f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_continuous_metrics(y_true, y_pred):\n",
    "    spearman_corr, _ = stats.spearmanr(y_true, y_pred)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "    bootstrap_spear = bootstrap(\n",
    "        data=(y_true, y_pred),\n",
    "        statistic=lambda y_t, y_p: stats.spearmanr(y_t, y_p)[0],\n",
    "        n_resamples=1000,\n",
    "        random_state=1,\n",
    "        method='basic'\n",
    "    )\n",
    "    ci_lower_spear, ci_upper_spear = bootstrap_spear.confidence_interval\n",
    "\n",
    "    bootstrap_rmse = bootstrap(\n",
    "        data=(y_true, y_pred),\n",
    "        statistic=lambda y_t, y_p: np.sqrt(np.mean((y_t - y_p) ** 2)),\n",
    "        n_resamples=1000,\n",
    "        random_state=1,\n",
    "        method='basic'\n",
    "    )\n",
    "    ci_lower_rmse, ci_upper_rmse = bootstrap_rmse.confidence_interval\n",
    "\n",
    "    return {\n",
    "        'spearman_corr': spearman_corr,\n",
    "        'rmse': rmse,\n",
    "        'ci_lower_spear': ci_lower_spear,\n",
    "        'ci_upper_spear': ci_upper_spear,\n",
    "        'ci_lower_rmse': ci_lower_rmse,\n",
    "        'ci_upper_rmse': ci_upper_rmse\n",
    "    }\n",
    "\n",
    "continuous_metrics_complete = calc_continuous_metrics(\n",
    "    y_true = validated_complete,\n",
    "    y_pred = llm_complete\n",
    ")\n",
    "continuous_metrics_complete = {k: round(v, 4) for k, v in continuous_metrics_complete.items()}\n",
    "print(\"Continuous metrics for complete cases:\")\n",
    "print(continuous_metrics_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331d40d",
   "metadata": {},
   "source": [
    "##### Context: to calculate RAI scores, zero-shot prompts were given to several different types of LLMs (no training done beyond the model's own pretraining). Even after several rounds of prompt engineering, some LLMs still occasionally failed to produce a valid output.\n",
    "\n",
    "**Question:** what is the best way to report model performance metrics in this context?\n",
    "\n",
    "**Proposal:** treat it like a classic missing data problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05737f",
   "metadata": {},
   "source": [
    "### Step 1: Characterize Missingness\n",
    "\n",
    "First, we need to decide if we can reasonably assume the data is:\n",
    "\n",
    "* MCAR: LLM failure probability has nothing do with any observed or unobserved patient features\n",
    "* MAR: LLM failure depends on patient feature(s), but not on patient's true frailty level\n",
    "* MNAR: LLM failure depends on the true frailty level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4070ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 4))\n",
    "plt.boxplot([validated_missing, validated_complete], \n",
    "            labels = ['Missing LLM Output', 'Complete LLM Output'])\n",
    "plt.title('Validated RAI Score by LLM Output Missingness')\n",
    "plt.ylabel('Validated RAI Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e65dbb1",
   "metadata": {},
   "source": [
    "The missingness pattern is unclear visually. Let's run a significance test to determine if there's a material difference between the two groups.\n",
    "\n",
    "First, let's do a visual check of the distributions of the two groups to see if we can use a t-test for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2,\n",
    "                         figsize = (10, 4), sharey = True)\n",
    "\n",
    "bins = np.linspace(scores_df['Validated_RAI_Score'].min(),\n",
    "                   scores_df['Validated_RAI_Score'].max(), 11)\n",
    "\n",
    "axes[0].hist(validated_complete, bins = bins, density = True)\n",
    "axes[0].set_title(\"Validated RAI Score (Complete LLM Output)\")\n",
    "axes[0].set_xlabel(\"RAI Score\")\n",
    "axes[0].set_ylabel(\"Probability Density\")\n",
    "\n",
    "axes[1].hist(validated_missing, bins = bins, density = True)\n",
    "axes[1].set_title(\"Validated RAI Score (Missing LLM Output)\")\n",
    "axes[1].set_xlabel(\"RAI Score\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea1232",
   "metadata": {},
   "source": [
    "Seems like not too skewed, but just in case let's run both a permutation test and a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Test for Difference in Means\n",
    "# Significance level: 0.05\n",
    "# Null: missing and complete RAI scores are from the same distribution\n",
    "# Alternative: different distributions\n",
    "def mean_difference(x, y, axis):\n",
    "    return np.mean(x, axis = axis) - np.mean(y, axis = axis)\n",
    "permtest = stats.permutation_test(data = (validated_complete,\n",
    "                                          validated_missing),\n",
    "                                    statistic = mean_difference,\n",
    "                                    vectorized = True)\n",
    "print(f'Permutation test p-value: {round(permtest.pvalue, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8834b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal Variances T-test for Difference in Means\n",
    "# Significance level: 0.05\n",
    "# Null: missing and complete RAI scores are from the same distribution\n",
    "# Alternative: different distributions\n",
    "ttest = stats.ttest_ind(validated_complete, validated_missing, equal_var = False)\n",
    "print(f'T-test p-value: {round(ttest.pvalue, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d2a435",
   "metadata": {},
   "source": [
    "Both of our tests show that at a 5% significance level, there's a difference between the two patient groups' RAI scores for complete vs missing LLM outputs. This means that the missingness is related to the outcome.\n",
    "\n",
    "This is a violation of the assumptions of MCAR and MAR, which are required to many statistical missing data methodologies. This means that we cannot use the complete cases for analysis without biasing our results. We should consider using imputation methods or sensitivity analysis to account for the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c8433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression between missingness and token_count\n",
    "X = sm.add_constant(scores_df['token_count'])\n",
    "y = scores_df['llm_RAI_score'].isna().astype(int)\n",
    "logit_model = sm.Logit(y, X)\n",
    "logit_result = logit_model.fit(disp = False)\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(logit_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b982f",
   "metadata": {},
   "source": [
    "Looks like there is a significant positive relationship between (log of) token count and missingness likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937b3f9",
   "metadata": {},
   "source": [
    "### Step 2: Identify Predictors of Missingness\n",
    "\n",
    "The first step when we have MNAR data is to attempt to get higher quality data. :) We did this with some prompt engineering, but still have some missing LLM outputs.\n",
    "\n",
    "To handle these, we first look at the associations between our true frailty scores and the patient features. The features we have access to are:\n",
    "\n",
    "* LLM-predicted RAI score (where our missingness occurs)\n",
    "* token count of clinical notes\n",
    "* patient age\n",
    "\n",
    "for each patient. As such, we'll investigate the following associations:\n",
    "\n",
    "1. LLM-predicted RAI score vs token count\n",
    "1. LLM-predicted RAI score vs age\n",
    "\n",
    "If any of these associations prove material, then we would impute the data based on the relationship between LLM-predicted RAI score and that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730190cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LLM-predicted RAI score against token count\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.scatter(scores_df['token_count'], scores_df['llm_RAI_score'], alpha = 0.5)\n",
    "plt.title('LLM-predicted RAI Score vs Token Count')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('LLM-predicted RAI Score')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression of LLM-predicted RAI score on token count\n",
    "olsmodel_token_count = sm.OLS(scores_df['llm_RAI_score'],\n",
    "                              sm.add_constant(scores_df[['token_count']]),\n",
    "                              missing = 'drop')\n",
    "olsfit_token_count   = olsmodel_token_count.fit()\n",
    "print(olsfit_token_count.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LLM-predicted RAI score against age\n",
    "plt.figure(figsize = (5, 4))\n",
    "plt.scatter(scores_df['age'], scores_df['llm_RAI_score'], alpha = 0.5)\n",
    "plt.title('LLM-predicted RAI Score vs Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('LLM-predicted RAI Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3863b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression of LLM-predicted RAI score on age\n",
    "olsmodel_age = sm.OLS(scores_df['llm_RAI_score'],\n",
    "                      sm.add_constant(scores_df['age']),\n",
    "                      missing = 'drop')\n",
    "olsfit_age   = olsmodel_age.fit()\n",
    "print(olsfit_age.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fac42",
   "metadata": {},
   "source": [
    "### Step 3: Imputation/Scenario Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44805d7a",
   "metadata": {},
   "source": [
    "##### Option 1: Deterministic Regression Imputation\n",
    "\n",
    "If either age or token count is a good linear predictor of llm_RAI_score, we can use that relationship to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"P-values for token count regression coefficients: {round(olsfit_token_count.pvalues[1], 4)}\")\n",
    "print(f\"P-values for age regression coefficients: {round(olsfit_age.pvalues[1], 4)}\")\n",
    "# token_count pval is below 0.05 significance level\n",
    "# age pval is not (? TBD based on the actual ages) below 0.05 significance level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b37a2",
   "metadata": {},
   "source": [
    "Token count looks to be a significant predictor of the LLM-predicted RAI score. We can thus use the regression results to predict llm_RAI_score for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = sm.add_constant(scores_df.loc[mask_missing, 'token_count'])\n",
    "imputed_token_counts = olsfit_token_count.predict(X_missing)\n",
    "\n",
    "imputed_scores_df = scores_df.copy()\n",
    "imputed_scores_df.loc[mask_missing, 'llm_RAI_score'] = imputed_token_counts\n",
    "\n",
    "imputed_scores_df['validated_frailty'] = (imputed_scores_df['Validated_RAI_Score'] >= 21)\n",
    "imputed_scores_df['llm_frailty']       = (imputed_scores_df['llm_RAI_score'] >= 21)\n",
    "\n",
    "print(imputed_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c19560",
   "metadata": {},
   "source": [
    "Then, we just rerun all our metrics on this imputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate binary classification metrics using function from before\n",
    "binary_metrics_linear_imputation = calc_binary_metrics(\n",
    "    y_true = imputed_scores_df['validated_frailty'],\n",
    "    y_pred = imputed_scores_df['llm_frailty']\n",
    ")\n",
    "binary_metrics_linear_imputation = {k: round(v, 4) for k, v in binary_metrics_linear_imputation.items()}\n",
    "print(\"Binary metrics for linearly imputed LLM RAI scores:\")\n",
    "print(binary_metrics_linear_imputation)\n",
    "\n",
    "print(\"Binary metrics for complete cases:\")\n",
    "print(binary_metrics_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c90e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate continuous metrics using function from before\n",
    "continuous_metrics_linear_imputation = calc_continuous_metrics(\n",
    "    y_true = imputed_scores_df['Validated_RAI_Score'],\n",
    "    y_pred = imputed_scores_df['llm_RAI_score']\n",
    ")\n",
    "continuous_metrics_linear_imputation = {k: round(v, 4) for k, v in continuous_metrics_linear_imputation.items()}\n",
    "print(\"Continuous metrics for linearly imputed LLM RAI scores:\")\n",
    "print(continuous_metrics_linear_imputation)\n",
    "\n",
    "print(\"Continuous metrics for complete cases:\")\n",
    "print(continuous_metrics_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bbd438",
   "metadata": {},
   "source": [
    "If age ends up being a factor as well, we can also run multiple linear regression and impute based on both age and token_count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec233ac",
   "metadata": {},
   "source": [
    "##### Option 2: Multiple Imputation\n",
    "\n",
    "If neither age or token count are good predictors of llm_RAI_score (or if we want to get a sort of best/worst case scenario range for our metrics), we then impute the data multiple times with random plausible values to get a range on our numbers. The bounds/imputation mechanisms are informed by our knowledge about RAI scores.\n",
    "\n",
    "We know:\n",
    "\n",
    "- min and max possible RAI scores (by definition between 0 and 81)\n",
    "- min and max RAI scores of our actual patients (0 and 47)\n",
    "\n",
    "Since we're not 100% about which of these bounds is more appropriate, let's perform a sensitivity analysis, i.e. try both sets to assess the impact on our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53650bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 100\n",
    "min_RAI, max_RAI = 0, 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566297c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_validated_RAI = min(scores_df['Validated_RAI_Score'])\n",
    "max_validated_RAI = max(scores_df['Validated_RAI_Score'])\n",
    "print(min_validated_RAI, max_validated_RAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85087208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_impute_with_randint(df, mask_missing, min_val, max_val, n_sims = 100):\n",
    "    imputed_dfs = []\n",
    "    for i in range(n_sims):\n",
    "        np.random.seed(i)\n",
    "        imputed_df = df.copy()\n",
    "        n_missing = mask_missing.sum()\n",
    "        imputed_vals = np.random.randint(min_val, max_val + 1, size = n_missing)\n",
    "        imputed_df.loc[mask_missing, 'llm_RAI_score'] = imputed_vals\n",
    "        imputed_df['llm_frailty'] = (imputed_df['llm_RAI_score'] >= 21)\n",
    "        imputed_df['validated_frailty'] = (imputed_df['Validated_RAI_Score'] >= 21)\n",
    "        imputed_dfs.append(imputed_df)\n",
    "    return imputed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1: theoretical min/max\n",
    "imputed_possible_all = mult_impute_with_randint(scores_df, mask_missing,\n",
    "                                                min_RAI, max_RAI, n_sims)\n",
    "# Version 2: min/max from validated RAI scores\n",
    "imputed_validated_all = mult_impute_with_randint(scores_df, mask_missing,\n",
    "                                                 min_validated_RAI,\n",
    "                                                 max_validated_RAI, n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2958213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imputed_metrics_samps(imputed_dfs):\n",
    "    spearman_samps, rmse_samps = [], []\n",
    "    accuracy_samps, precision_samps, recall_samps = [], [], []\n",
    "    f1_samps, pr_auc_samps = [], []\n",
    "\n",
    "    for imputed_df in imputed_dfs:\n",
    "        cont_met = calc_continuous_metrics(\n",
    "            y_true = imputed_df['Validated_RAI_Score'],\n",
    "            y_pred = imputed_df['llm_RAI_score']\n",
    "        )\n",
    "        bin_met = calc_binary_metrics(\n",
    "            y_true = imputed_df['validated_frailty'],\n",
    "            y_pred = imputed_df['llm_frailty']\n",
    "        )\n",
    "        spearman_samps.append(cont_met['spearman_corr'])\n",
    "        rmse_samps.append(cont_met['rmse'])\n",
    "        accuracy_samps.append(bin_met['accuracy'])\n",
    "        precision_samps.append(bin_met['precision'])\n",
    "        recall_samps.append(bin_met['recall'])\n",
    "        f1_samps.append(bin_met['f1_score'])\n",
    "        pr_auc_samps.append(bin_met['pr_auc'])\n",
    "    return spearman_samps, rmse_samps, accuracy_samps, precision_samps, recall_samps, f1_samps, pr_auc_samps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05548a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo model:\n",
    "spearman_samps_possible, rmse_samps_possible, accuracy_samps_possible, precision_samps_possible, recall_samps_possible, f1_samps_possible, pr_auc_samps_possible = get_imputed_metrics_samps(imputed_possible_all)\n",
    "spearman_samps_validated, rmse_samps_validated, accuracy_samps_validated, precision_samps_validated, recall_samps_validated, f1_samps_validated, pr_auc_samps_validated = get_imputed_metrics_samps(imputed_validated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ba189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo model:\n",
    "print(\"Theoretical min/max imputation metrics:\")\n",
    "print(f\"Accuracy: {round(np.mean(accuracy_samps_possible), 4)}\")\n",
    "print(f\"Precision: {round(np.mean(precision_samps_possible), 4)}\")\n",
    "print(f\"Recall: {round(np.mean(recall_samps_possible), 4)}\")\n",
    "print(f\"F1 Score: {round(np.mean(f1_samps_possible), 4)}\")\n",
    "print(f\"PR AUC: {round(np.mean(pr_auc_samps_possible), 4)}\")\n",
    "print(f\"Spearman correlation: {round(np.mean(spearman_samps_possible), 4)}\")\n",
    "print(f\"RMSE: {round(np.mean(rmse_samps_possible), 4)}\")\n",
    "\n",
    "print(\"\\nValidated RAI min/max imputation metrics:\")\n",
    "print(f\"Accuracy: {round(np.mean(accuracy_samps_validated), 4)}\")\n",
    "print(f\"Precision: {round(np.mean(precision_samps_validated), 4)}\")\n",
    "print(f\"Recall: {round(np.mean(recall_samps_validated), 4)}\")\n",
    "print(f\"F1 Score: {round(np.mean(f1_samps_validated), 4)}\")\n",
    "print(f\"PR AUC: {round(np.mean(pr_auc_samps_validated), 4)}\")\n",
    "print(f\"Spearman correlation: {round(np.mean(spearman_samps_validated), 4)}\")\n",
    "print(f\"RMSE: {round(np.mean(rmse_samps_validated), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be6f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all models:\n",
    "all_model_names = [col for col in all_df.columns if col.endswith('agent_RAI') or col.endswith('single_RAI')]\n",
    "all_model_names.sort()\n",
    "print(all_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8072c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_metrics(metrics):\n",
    "    return {k: round(v, 4) for k, v in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_for_all_models(all_df, all_model_names, min_val, max_val, n_sims = 100):\n",
    "    for model_name in all_model_names:\n",
    "        scores_df = get_specific_model_df(all_df, model_name)\n",
    "        n = len(scores_df)\n",
    "        n_missing = sum(scores_df['llm_RAI_score'].isna())\n",
    "        n_complete = n - n_missing\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Number of complete cases: {n_complete}, Number of missing cases: {n_missing}, Total: {n}\")\n",
    "        print(f\"Failure percentage for {model_name}: {(n_missing / n) * 100:.2f}%\")\n",
    "\n",
    "        if n_missing == 0:\n",
    "            print(f\"No missing cases for {model_name}. Skipping imputation.\")\n",
    "            scores_df['validated_frailty'] = (scores_df['Validated_RAI_Score'] >= 21)\n",
    "            scores_df['llm_frailty']       = (scores_df['llm_RAI_score'] >= 21)\n",
    "            binary_metrics_complete = calc_binary_metrics(\n",
    "                y_true = scores_df['validated_frailty'],\n",
    "                y_pred = scores_df['llm_frailty']\n",
    "            )\n",
    "            continuous_metrics_complete = calc_continuous_metrics(\n",
    "                y_true = scores_df['Validated_RAI_Score'],\n",
    "                y_pred = scores_df['llm_RAI_score']\n",
    "            )\n",
    "            print(f\"Binary metrics for {model_name}:\")\n",
    "            print(round_metrics(binary_metrics_complete))\n",
    "            print(f\"Continuous metrics for {model_name}:\")\n",
    "            print(round_metrics(continuous_metrics_complete))\n",
    "        \n",
    "        else:\n",
    "            print(f\"Missing cases found for {model_name}. Proceeding with imputation.\")\n",
    "            mask_missing = scores_df[\"llm_RAI_score\"].isna()\n",
    "            imputed_df_all = mult_impute_with_randint(scores_df, mask_missing, min_val, max_val, n_sims)\n",
    "            spearman_samps, rmse_samps, accuracy_samps, precision_samps, recall_samps, f1_samps, pr_auc_samps = get_imputed_metrics_samps(imputed_df_all)\n",
    "            print(f\"Imputation metrics for {model_name}:\")\n",
    "            print(f\"Spearman: {np.mean(spearman_samps):.4f}, \"\n",
    "                f\"RMSE: {np.mean(rmse_samps):.4f}\")\n",
    "            print(f\"Acc: {np.mean(accuracy_samps):.4f}, \"\n",
    "                f\"Prec: {np.mean(precision_samps):.4f}, \"\n",
    "                f\"Recall: {np.mean(recall_samps):.4f}, \"\n",
    "                f\"F1 Score: {np.mean(f1_samps):.4f}, \"\n",
    "                f\"PR AUC: {np.mean(pr_auc_samps):.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 81\n",
    "print_metrics_for_all_models(all_df, all_model_names, min_RAI, max_RAI, n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bccd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 47\n",
    "print_metrics_for_all_models(all_df, all_model_names, min_validated_RAI, max_validated_RAI, n_sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
